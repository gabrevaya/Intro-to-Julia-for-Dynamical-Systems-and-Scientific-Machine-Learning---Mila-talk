{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia for Dynamical Systems and Scientific Machine Learning\n",
    "\n",
    "### Julia ecosystems of scientific computing\n",
    "\n",
    "There are many Julia organizations about seemingly every scientific area. Some examples:\n",
    "\n",
    "- Astrophysics: [JuliaAstro](https://juliaastro.github.io/), [JuliaSpace](https://github.com/JuliaSpace)\n",
    "- Bio/Chemistry: [JuliaBio](https://biojulia.net/), [Molecular simulations](https://github.com/JuliaMolSim)\n",
    "- Complex systems, nonlinear dynamics: [JuliaDynamics](https://juliadynamics.github.io/JuliaDynamics/)\n",
    "- Scientific Machine Learning: [SciML](https://juliadiffeq.org/)\n",
    "- Solid state: [QuantumOptics](https://qojulia.org/), [JuliaPhysics](https://github.com/JuliaPhysics)\n",
    "- Economics: [QuantEcon](https://julia.quantecon.org/), [JuliaQuant](https://github.com/JuliaQuant)\n",
    "- Geosciences/Climate: [JuliaGeo](https://github.com/JuliaGeo), [JuliaEarth](https://github.com/JuliaEarth), [JuliaClimate](https://github.com/JuliaClimate), [CliMA](https://github.com/CliMA)\n",
    "- Optimization: [JuliaOpt](http://www.juliaopt.org/)\n",
    "- Machine Learning: [JuliaML](https://juliaml.github.io/), [Flux](https://fluxml.ai/)\n",
    "\n",
    "among many many others...\n",
    "\n",
    "For discovering more packages you can take a look at this [curated list](https://github.com/svaksha/Julia.jl) (although some of the topics are a bit outdated), to the [JuliaHub](https://juliahub.com/) or checking out the [JuliaCon's playlists](https://www.youtube.com/user/JuliaLanguage/playlists).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this while we are working on Atom to save time\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.instantiate()\n",
    "using DifferentialEquations, Plots, Interact, DynamicalSystems, StaticArrays, LinearAlgebra\n",
    "using DataDrivenDiffEq, ModelingToolkit, DiffEqFlux, DiffEqSensitivity, Flux, Optim, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DifferentialEquations.jl\n",
    "\n",
    "[DifferentialEquations.jl](https://docs.juliadiffeq.org/latest/) is by far [the best](https://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/) free and open source differential equations solver (not for Julia, for any language). It can solve standard ODEs, Delay-DEs, stochastic DEs, has tools for PDEs, event handling, other multiple features, 100s of solvers, and *even more*.\n",
    "\n",
    "\n",
    "## Defining and solving some ODEs\n",
    "\n",
    "The way DifferentialEquations.jl works is quite straightforward:\n",
    "\n",
    "1. Make your set of ODEs a Julia function `f`\n",
    "2. Put `f`, an initial state, timespan and a parameter container into an `ODEProblem`.\n",
    "2. Optionaly choose the solvers and the arguments of the solvers you will use (e.g. tolerances, etc.)\n",
    "3. Give `f` and auxilary arguments to the function `solve`!\n",
    "\n",
    "Let's see it in practice by solving the Lorenz system\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{x} &= \\sigma (y - x) \\\\\n",
    "\\dot{y} &= x (\\rho - z) - y \\\\\n",
    "\\dot{z} &= xy -\\beta z\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function lorenz(u,p,t)                        # define the system\n",
    "    x, y, z = u\n",
    "    œÉ, œÅ, Œ≤ = p\n",
    "    \n",
    "    ·∫ã = œÉ*(y - x)\n",
    "    ·∫è = x*(œÅ - z) - y\n",
    "    ≈º = x*y - Œ≤*z\n",
    "    \n",
    "    return [·∫ã, ·∫è, ≈º]\n",
    "end\n",
    "                                \n",
    "u‚ÇÄ = [1.0, 5.0, 10.0]                         # initial conditions\n",
    "p = [10, 28, 8/3]                             # parameters        \n",
    "tspan = (0.0, 100.0)                          # timespan\n",
    "prob = ODEProblem(lorenz, u‚ÇÄ, tspan, p)       # define the problem\n",
    "sol = solve(prob)                             # solve it\n",
    "plot(sol, vars = (1, 2, 3))                   # plot the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzt = plot(sol)\n",
    "xy = plot(sol, vars=(1,2))\n",
    "xz = plot(sol, vars=(1,3))\n",
    "yz = plot(sol, vars=(2,3))\n",
    "xyz = plot(sol, vars=(1,2,3))\n",
    "\n",
    "plot(plot(xyzt,xyz), plot(xy, xz, yz, layout=(1,3)), layout=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick example, now providing the equations in-place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra!(du,u,p,t)                        # define the system\n",
    "    üêá, ü¶ä = u\n",
    "    Œ±, Œ≤, Œ≥, Œ¥ = p\n",
    "    \n",
    "    du[1] = düêá =  Œ±*üêá - Œ≤*üêá*ü¶ä\n",
    "    du[2] = dü¶ä = -Œ≥*ü¶ä + Œ¥*üêá*ü¶ä\n",
    "end\n",
    "                                        \n",
    "≈©‚ÇÄ = [1.0, 1.0]                                           # initial conditions\n",
    "timespan = (0.0, 10.0)                                    # timespan\n",
    "pÃÉ = [1.5, 1.0, 3.0, 1.0]                                  # parameters\n",
    "prob = ODEProblem(lotka_volterra!, ≈©‚ÇÄ, timespan, pÃÉ)       # define the problem\n",
    "sol_lv = solve(prob)                                      # solve it\n",
    "plot(sol_lv, label = [\"Rabbits\" \"Foxes\"])                 # plot solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@manipulate for Œ± ‚àà 0:0.1:5, Œ≤ ‚àà 0:0.1:5\n",
    "    pÃÉ = [Œ± Œ≤ 3.0 1.0]\n",
    "    prob = ODEProblem(lotka_volterra!, ≈©‚ÇÄ, timespan, pÃÉ)\n",
    "    sol_lv = solve(prob)\n",
    "    plot(sol_lv, label = [\"Rabbits\" \"Foxes\"])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/SciML/DiffEqTutorials.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DynamicalSystems.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DynamicalSystems.jl is an award-winning library for nonlinear dynamics and chaos, part of [JuliaDynamics](https://juliadynamics.github.io/JuliaDynamics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"map_of_ds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using known system $f$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dynamical system where you know its rule $f$, there are a lot of things you can do with in DynamicalSystems.jl. But for all of them, step 1 is to make this rule $f$ into a `DynamicalSystem` instance. The process here is almost identical to DifferentialEquations.jl:\n",
    "\n",
    "1. Make $f$ a Julia function (using the same syntax as in DifferentialEquations.jl).\n",
    "2. Choose initial state and parameter container (again same as DiffEq).\n",
    "3. Pass these arguments to the `ContinuousDynamicalSystem` constructor (or `DiscreteDynamicalSystem`, for discrete systems).\n",
    "\n",
    "Here we already have `lorenz, u‚ÇÄ, p` defined, so the process would be just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorenz_ds = ContinuousDynamicalSystem(lorenz, u‚ÇÄ, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, DynamicalSystems.jl requires that we use StaticArrays when we provide an out-of-place equation of motion, which are similar to usual arrays but are stack-allocated and thus are more performant for small systems ( $\\lesssim$ 100 ODEs/SDEs/DDEs/DAEs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lorenz(u,p,t)\n",
    "    x, y, z = u\n",
    "    œÉ, œÅ, Œ≤ = p\n",
    "    \n",
    "    ·∫ã = œÉ*(y - x)\n",
    "    ·∫è = x*(œÅ - z) - y\n",
    "    ≈º = x*y - Œ≤*z\n",
    "    \n",
    "    return SVector(·∫ã, ·∫è, ≈º)\n",
    "end\n",
    "                                \n",
    "static_u‚ÇÄ = SVector(1.0, 5., 10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorenz_ds = ContinuousDynamicalSystem(lorenz, static_u‚ÇÄ, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this `lorenz` object in various functions of the library. For example, to obtain the Lyapunov spectrum you use `lyapunovs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunovs(lorenz_ds, 10000) # 2nd argument is for how much time to evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to get only the maximum Lyapunov exponent you can use the function `lyapunov` (which uses a different algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyapunov(lorenz_ds, 10000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a Poincare surface of section with the plane that e.g. the $y$ variable crosses zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane = (2, 0.0) # plane[1]: which variable, plane[2]: which value\n",
    "psos = poincaresos(lorenz_ds, plane, 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(psos[:, 1], psos[:, 3], label = false)\n",
    "xlabel!(\"X\")\n",
    "ylabel!(\"Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear timeseries analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many nonlinear timeseries analysis tools are included in DynamicalSystems.jl. such as Broomhead-King delay embeddings, time series prediction, calculating the generalized (R√©nyi) and permutation entropy, many different kinds attractor dimensions (like fractal dimension, Grassberger-Proccacia dimension, Kaplan-Yorke dimension), etc. \n",
    "\n",
    "Let's say that you have a nice trajectory sampled at discrete times (which is what the function `trajectory` does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = trajectory(lorenz_ds, 100.0; dt = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of calculating the [generalized entropy](https://juliadynamics.github.io/DynamicalSystems.jl/dev/chaos/entropies/#Generalized-Entropy-1) of the attractor (which is calculated by partitioning the state space in boxes of size `Œµ`).\n",
    "\n",
    "You can get this entropy (of order Œ±) using the function `genentropy(Œ±, Œµ, tr)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = genentropy(1, 0.1, tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = genentropy(1, 0.01, tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quantity is interesting because it can be used to calculate the fractal dimension of the `tr` object. The way it works is simple: You look at how `H` changes for varying `Œµ`. If `D` is the fractal dimension of `tr`, then it typically holds that $H \\sim Œµ^{-D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_lorenz = generalized_dim(1, tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DataDrivenDiffEq.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DataDrivenDiffEq.jl](https://datadriven.sciml.ai/dev/) is a package for estimating equation-free and equation-based models for discrete and continuous differential equations.\n",
    "\n",
    "As opposed to parameter identification, these methods aim to find the governing equations of motion automatically from a given set of data. They do not require a known model as input. Instead, these methods take in data and return the differential equation model which generated the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Identification of Nonlinear Dynamics (SINDy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SINDy.png\">\n",
    "\n",
    "(extracted from the original [SINDy paper](https://www.pnas.org/content/113/15/3932))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with the Lorenz system, that we've been working on. First let's get the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Array(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the *ideal* derivative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "·∫ä = similar(X)\n",
    "for (i, xi) in enumerate(eachcol(X))\n",
    "    ·∫ä[:,i] = lorenz(xi, p, 0.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the symbolic equations, we need to define a ` Basis` over the variables `x y z`. In this example, we will use all monomials up to degree of 4 and their products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variables x y z\n",
    "u = Operation[x; y; z]\n",
    "polys = Operation[]\n",
    "for i ‚àà 0:4\n",
    "    for j ‚àà 0:4\n",
    "        for  k ‚àà 0:4\n",
    "            push!(polys, u[1]^i * u[2]^j * u[3]^k)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "basis = Basis(polys, u)\n",
    "print(basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we could also add more functions to the basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [cos(u[1]); sin(u[1]); u[1]*sin(u[2]); u[2]*cos(u[2])]\n",
    "\n",
    "push!(basis, h)\n",
    "print(basis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can select an optimizier, and run the SINDy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = STRRidge(0.1)\n",
    "Œ® = SInDy(X, ·∫ä, basis, maxiter = 100, opt = opt, normalize = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_equations(Œ®)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = parameters(Œ®)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the ``L2``-Error and Akaikes Information Criterion of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_error(Œ®)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_aicc(Œ®)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a numerical model usable in `DifferentialEquations`, we simply use the `ODESystem` function from `ModelingToolkit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = parameters(Œ®)\n",
    "sys = ODESystem(Œ®)\n",
    "dudt = ODEFunction(sys)\n",
    "\n",
    "prob = ODEProblem(dudt, u‚ÇÄ, tspan, ps)\n",
    "solution = solve(prob, saveat = sol.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the trajectory of $x(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(solution, vars=1, label = \"Estimation\")\n",
    "plot!(sol, vars = 1, label = \"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's investigate the error of the chaotic equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "œµ = norm.(eachcol(solution .- sol))\n",
    "plot(solution.t, œµ .+ eps(), yaxis = :log, legend = false)\n",
    "xlabel!(\"Time [s]\")\n",
    "ylabel!(\"L2 Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DiffEqFlux.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DiffEqFlux.jl](https://diffeqflux.sciml.ai/) is another really exciting package from the [Scientific Machine Learning Ecosystem](https://sciml.ai). Besides implementing NeuralODEs/SDEs/DDEs, etc they provide tools for builing what they have called [Universal Differential Equations](https://arxiv.org/abs/2001.04385).\n",
    "\n",
    "What if we only know some part of the differential equations?\n",
    "Could we plug-in some neural networks to infer the missing terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra(u, p,t)\n",
    "    üêá, ü¶ä = u\n",
    "    Œ±, Œ≤, Œ≥, Œ¥ = p\n",
    "\n",
    "    düêá =  Œ±*üêá - Œ≤*üêá*ü¶ä\n",
    "    dü¶ä = -Œ≥*ü¶ä + Œ¥*üêá*ü¶ä\n",
    "    return [düêá, dü¶ä]\n",
    "end\n",
    "\n",
    "tspan = (0.0, 3.0)\n",
    "u‚ÇÄ = [0.4, 4.6]\n",
    "p‚ÇÄ = [1.3, 0.9, 1.8, 0.8]\n",
    "prob = ODEProblem(lotka_volterra, u‚ÇÄ, tspan, p‚ÇÄ)\n",
    "solution = solve(prob, Vern7(), abstol=1e-12, reltol=1e-12, saveat = 0.1)\n",
    "\n",
    "scatter(solution, alpha = 0.25, label = false)\n",
    "plot!(solution, alpha = 0.5, label = [\"Rabbits\" \"Foxes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some noisy data from the true solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal data\n",
    "tsdata = Array(solution)\n",
    "# Add noise to the data\n",
    "Random.seed!(333)\n",
    "noisy_data = tsdata + 1e-5*randn(eltype(tsdata), size(tsdata))\n",
    "\n",
    "plot(abs.(tsdata - noisy_data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we actually don't know the underlying equations but we know or can guess some parts of it. We could use an artificial neural network to learn the missing terms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{x} &= \\alpha x + NN_1(x, y) \\\\\n",
    "\\dot{y} &= - \\gamma y + NN_2(x, y)\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra_univ(u,p,t)\n",
    "    üêá, ü¶ä = u\n",
    "    Œ± = p‚ÇÄ[1]\n",
    "    Œ≥ = p‚ÇÄ[3]\n",
    "    NN‚ÇÅ, NN‚ÇÇ = ann(u, p)\n",
    "\n",
    "    düêá =  Œ±*üêá + NN‚ÇÅ\n",
    "    dü¶ä = -Œ≥*ü¶ä + NN‚ÇÇ\n",
    "\n",
    "    return [düêá, dü¶ä]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neueral network for learning the parts\n",
    "# of the equations that we don't know\n",
    "ann = FastChain(FastDense(2, 32, tanh),\n",
    "                FastDense(32, 32, tanh),\n",
    "                FastDense(32, 2))\n",
    "\n",
    "# The model weights are destructured into a vector of parameters\n",
    "p = initial_params(ann)\n",
    "\n",
    "prob_univ = ODEProblem(lotka_volterra_univ, u‚ÇÄ, tspan, p)\n",
    "sol_univ = concrete_solve(prob_univ, Tsit5(), u‚ÇÄ, p, saveat = solution.t)\n",
    "\n",
    "# plot the true solution and \n",
    "plot(solution)\n",
    "plot!(sol_univ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prediction function and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(Œ∏)\n",
    "    Array(concrete_solve(prob_univ, Vern7(), u‚ÇÄ, Œ∏, saveat = solution.t,\n",
    "                         abstol=1e-6, reltol=1e-6,\n",
    "                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP())))\n",
    "end\n",
    "\n",
    "# No regularisation right now\n",
    "function loss(Œ∏)\n",
    "    pred = predict(Œ∏)\n",
    "    sum(abs2, noisy_data .- pred), pred # + 1e-5*sum(sum.(abs, params(Œ∏)))\n",
    "end\n",
    "\n",
    "loss(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback for printing the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const losses = []\n",
    "callback(Œ∏,l,pred) = begin\n",
    "    push!(losses, l)\n",
    "    if length(losses)%50==0\n",
    "        println(losses[end])\n",
    "    end\n",
    "    false\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train it. First let's use ADAM for some iterations and then refine it with BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = DiffEqFlux.sciml_train(loss, res1.minimizer, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "plot(losses, yaxis = :log, xaxis = :log, xlabel = \"Iterations\", ylabel = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and the approximation\n",
    "NNsolution = predict(res2.minimizer)\n",
    "# Trained on noisy data vs real solution\n",
    "plot(solution.t, tsdata')\n",
    "plot!(solution.t, NNsolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the state trajectory and the derivatives\n",
    "X = noisy_data\n",
    "# Ideal derivatives\n",
    "DX = Array(solution(solution.t, Val{1})) #- [p[1]*(X[1,:])';  -p[4]*(X[2,:])']\n",
    "\n",
    "prob_univ2 = ODEProblem(lotka_volterra_univ, u‚ÇÄ, tspan, res2.minimizer)\n",
    "_sol = solve(prob_univ2, Tsit5())\n",
    "\n",
    "DX_ = Array(_sol(solution.t, Val{1}))\n",
    "\n",
    "# The learned derivatives\n",
    "plot(DX')\n",
    "plot!(DX_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True missing terms\n",
    "L = [-p‚ÇÄ[2]*(X[1,:].*X[2,:])'; p‚ÇÄ[4]*(X[1,:].*X[2,:])']\n",
    "\n",
    "# Learned\n",
    "LÃÇ = ann(X,res2.minimizer)\n",
    "\n",
    "scatter(L')\n",
    "plot!(LÃÇ')\n",
    "\n",
    "# scatter(abs.(L-LÃÇ)', yaxis = :log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extend the time span of the integration and see how it performs for long term predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's see some long term predictions\n",
    "tspan_long = (0.0f0, 30.0f0)\n",
    "prob = ODEProblem(lotka_volterra, u‚ÇÄ, tspan_long, p‚ÇÄ)\n",
    "solution = solve(prob, Vern7(), abstol=1e-6, reltol=1e-6)\n",
    "\n",
    "prob_univ = ODEProblem(lotka_volterra_univ, u‚ÇÄ, tspan_long, res2.minimizer)\n",
    "sol_univ = solve(prob, Vern7(), abstol=1e-6, reltol=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "scatter(sol_univ, alpha = 0.25, label = [\"Predicted Rabbits\" \"Predicted Foxes\"])\n",
    "plot!(solution, label = [\"True Rabbits\" \"True Foxes\"])\n",
    "plot!([2.99,3.01],[0.0, maximum(hcat(Array(solution), Array(sol_univ)))],\n",
    "      lw=2, color=:black, label = false)\n",
    "annotate!([(1.5,9, text(\"Training \\nData\", 10, :center, :top, :black, \"Helvetica\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "89034b82a0bd4a64811dabcbb428b736",
   "lastKernelId": "25c0468f-416d-4b81-a616-32be5dd63204"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
